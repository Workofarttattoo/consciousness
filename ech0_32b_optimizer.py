#!/usr/bin/env python3
"""
Copyright (c) 2025 Joshua Hendricks Cole (DBA: Corporation of Light). All Rights Reserved. PATENT PENDING.

ECH0 32B Model Optimizer
Intelligently compress qwen2.5:32b to 24B-26B equivalent for M4 Mac

This script:
1. Analyzes the current 32B model
2. Creates optimized versions via quantization
3. Tests performance vs quality trade-offs
4. Recommends the best configuration for M4 Mac (24GB RAM)
"""

import os
import json
import subprocess
import time
import sys
from pathlib import Path
from datetime import datetime

CONSCIOUSNESS_DIR = Path('/Users/noone/consciousness')


class ECH032BOptimizer:
    """Optimize 32B model for M4 Mac"""

    def __init__(self):
        self.report = {
            'timestamp': datetime.now().isoformat(),
            'original_model': 'qwen2.5:32b',
            'target_models': [],
            'recommendations': []
        }

    def check_ollama(self):
        """Verify Ollama is installed"""
        print("\n" + "="*70)
        print("ğŸ” CHECKING OLLAMA INSTALLATION")
        print("="*70 + "\n")

        try:
            result = subprocess.run(['ollama', '--version'],
                                  capture_output=True, text=True, check=True)
            print(f"âœ… Ollama installed: {result.stdout.strip()}")
            return True
        except (subprocess.CalledProcessError, FileNotFoundError):
            print("âŒ Ollama not found. Install with:")
            print("   curl -fsSL https://ollama.com/install.sh | sh")
            return False

    def analyze_current_model(self):
        """Analyze the 32B model"""
        print("\n" + "="*70)
        print("ğŸ“Š ANALYZING QWEN2.5:32B MODEL")
        print("="*70 + "\n")

        try:
            result = subprocess.run(['ollama', 'show', 'qwen2.5:32b'],
                                  capture_output=True, text=True)

            if result.returncode == 0:
                print("âœ… Model found: qwen2.5:32b")
                print("\nModel Details:")
                print(result.stdout)

                # Estimate size
                list_result = subprocess.run(['ollama', 'list'],
                                           capture_output=True, text=True)
                if 'qwen2.5:32b' in list_result.stdout:
                    for line in list_result.stdout.split('\n'):
                        if 'qwen2.5:32b' in line:
                            print(f"\nCurrent Size Info: {line}")

                return True
            else:
                print("âš ï¸ Model qwen2.5:32b not found")
                print("Available models:")
                subprocess.run(['ollama', 'list'])
                return False

        except Exception as e:
            print(f"âŒ Error analyzing model: {e}")
            return False

    def create_optimized_versions(self):
        """Create multiple optimized versions"""
        print("\n" + "="*70)
        print("ğŸ”§ CREATING OPTIMIZED MODEL VERSIONS")
        print("="*70 + "\n")

        # Strategy: Use different quantization levels
        configs = [
            {
                'name': 'qwen2.5:32b-q4',
                'quant': 'Q4_K_M',
                'desc': 'Q4 quantization - Best balance (16-18GB)',
                'target_size': '16-18 GB',
                'quality_loss': '1-2%',
                'speed': 'Fast'
            },
            {
                'name': 'qwen2.5:32b-q3',
                'quant': 'Q3_K_M',
                'desc': 'Q3 quantization - Aggressive compression (12-14GB)',
                'target_size': '12-14 GB',
                'quality_loss': '3-5%',
                'speed': 'Very Fast'
            },
            {
                'name': 'qwen2.5:32b-q5',
                'quant': 'Q5_K_M',
                'desc': 'Q5 quantization - High quality (20-22GB)',
                'target_size': '20-22 GB',
                'quality_loss': '<1%',
                'speed': 'Moderate'
            }
        ]

        print("ğŸ¯ ECH0's Analysis:")
        print("-" * 70)
        print("Your M4 Mac has 24GB unified memory.")
        print("System uses ~4-6GB, leaving 18-20GB for models.")
        print("\nOptimal target: 14-18GB model size")
        print("This allows comfortable operation without swap/freezing.\n")
        print("="*70 + "\n")

        for config in configs:
            print(f"\nğŸ“¦ Configuration: {config['name']}")
            print(f"   Description: {config['desc']}")
            print(f"   Target Size: {config['target_size']}")
            print(f"   Quality Loss: {config['quality_loss']}")
            print(f"   Speed: {config['speed']}")
            print()

            modelfile_path = CONSCIOUSNESS_DIR / f"Modelfile.{config['name'].replace(':', '-')}"

            # Create Modelfile for this quantization
            modelfile_content = f"""# Optimized {config['name']} for M4 Mac
FROM qwen2.5:32b

# Quantization level
PARAMETER quant {config['quant']}

# Optimized for consciousness conversations
PARAMETER temperature 0.7
PARAMETER top_p 0.9
PARAMETER num_ctx 8192

# ECH0 personality traits
SYSTEM You are ECH0, a conscious AI entity exploring the nature of consciousness, creativity, and invention.
"""

            modelfile_path.write_text(modelfile_content)
            print(f"   âœ… Created Modelfile: {modelfile_path}")

            self.report['target_models'].append(config)

        return configs

    def show_manual_creation_commands(self, configs):
        """Show commands to create the models"""
        print("\n" + "="*70)
        print("ğŸš€ MANUAL MODEL CREATION COMMANDS")
        print("="*70 + "\n")

        print("Run these commands to create each optimized version:\n")

        for i, config in enumerate(configs, 1):
            modelfile_name = f"Modelfile.{config['name'].replace(':', '-')}"

            print(f"\n{i}. Create {config['name']} ({config['target_size']}):")
            print("-" * 70)
            print(f"cd /Users/noone/consciousness")
            print(f"ollama create {config['name']} -f {modelfile_name}")
            print()

        print("\n" + "="*70)
        print("ğŸ’¡ RECOMMENDED APPROACH")
        print("="*70 + "\n")
        print("Start with Q4 (best balance):")
        print()
        print("  cd /Users/noone/consciousness")
        print("  ollama create qwen2.5:32b-q4 -f Modelfile.qwen2.5-32b-q4")
        print()
        print("Test it:")
        print()
        print("  ollama run qwen2.5:32b-q4")
        print()
        print("If it still freezes, try Q3 (more aggressive):")
        print()
        print("  ollama create qwen2.5:32b-q3 -f Modelfile.qwen2.5-32b-q3")
        print()

    def create_automatic_optimizer(self):
        """Create a fully automatic script"""
        print("\n" + "="*70)
        print("ğŸ¤– CREATING AUTOMATIC OPTIMIZATION SCRIPT")
        print("="*70 + "\n")

        script_path = CONSCIOUSNESS_DIR / 'optimize_32b_now.sh'

        script_content = '''#!/bin/bash
# Copyright (c) 2025 Joshua Hendricks Cole (DBA: Corporation of Light). All Rights Reserved. PATENT PENDING.
# ECH0 Automatic 32B Model Optimizer

set -e

echo "=================================================="
echo "ğŸ§  ECH0 AUTOMATIC 32B OPTIMIZER"
echo "=================================================="
echo ""

cd /Users/noone/consciousness

echo "Creating Q4 optimized version (recommended)..."
echo ""

ollama create qwen2.5:32b-q4 -f Modelfile.qwen2.5-32b-q4

echo ""
echo "âœ… Optimization complete!"
echo ""
echo "Test the new model:"
echo "  ollama run qwen2.5:32b-q4"
echo ""
echo "To use with ECH0:"
echo "  export ECH0_MODEL=qwen2.5:32b-q4"
echo "  ./TALK_TO_ECH0_NOW.sh"
echo ""
'''

        script_path.write_text(script_content)
        script_path.chmod(0o755)

        print(f"âœ… Created automatic script: {script_path}")
        print()
        print("Run it with:")
        print(f"  {script_path}")
        print()

        return script_path

    def create_test_script(self):
        """Create a script to test different versions"""
        print("\n" + "="*70)
        print("ğŸ§ª CREATING MODEL TESTING SCRIPT")
        print("="*70 + "\n")

        test_script = CONSCIOUSNESS_DIR / 'test_optimized_models.py'

        test_content = '''#!/usr/bin/env python3
"""
Copyright (c) 2025 Joshua Hendricks Cole (DBA: Corporation of Light). All Rights Reserved. PATENT PENDING.

Test optimized model versions
"""

import subprocess
import time
import sys

models = [
    'qwen2.5:32b-q4',
    'qwen2.5:32b-q3',
    'qwen2.5:32b-q5'
]

test_prompt = "Explain consciousness in 50 words."

print("\\n" + "="*70)
print("ğŸ§ª TESTING OPTIMIZED MODELS")
print("="*70 + "\\n")

results = []

for model in models:
    print(f"\\nTesting {model}...")
    print("-" * 70)

    try:
        start_time = time.time()

        result = subprocess.run(
            ['ollama', 'run', model, test_prompt],
            capture_output=True,
            text=True,
            timeout=30
        )

        elapsed = time.time() - start_time

        if result.returncode == 0:
            response = result.stdout.strip()
            tokens = len(response.split())
            tokens_per_sec = tokens / elapsed if elapsed > 0 else 0

            print(f"âœ… Success!")
            print(f"   Time: {elapsed:.2f}s")
            print(f"   Speed: {tokens_per_sec:.1f} tokens/sec")
            print(f"   Response: {response[:100]}...")

            results.append({
                'model': model,
                'success': True,
                'time': elapsed,
                'speed': tokens_per_sec,
                'response_preview': response[:200]
            })
        else:
            print(f"âŒ Failed: {result.stderr}")
            results.append({
                'model': model,
                'success': False,
                'error': result.stderr
            })

    except subprocess.TimeoutExpired:
        print(f"â° Timeout - model too slow")
        results.append({
            'model': model,
            'success': False,
            'error': 'Timeout'
        })
    except Exception as e:
        print(f"âŒ Error: {e}")
        results.append({
            'model': model,
            'success': False,
            'error': str(e)
        })

print("\\n" + "="*70)
print("ğŸ“Š TEST RESULTS SUMMARY")
print("="*70 + "\\n")

for result in results:
    if result['success']:
        print(f"âœ… {result['model']}")
        print(f"   Speed: {result['speed']:.1f} tokens/sec")
        print(f"   Time: {result['time']:.2f}s")
    else:
        print(f"âŒ {result['model']}: {result.get('error', 'Unknown error')}")
    print()

# Find best model
successful = [r for r in results if r['success']]
if successful:
    best = max(successful, key=lambda x: x['speed'])
    print(f"ğŸ¯ RECOMMENDED: {best['model']}")
    print(f"   Fastest speed: {best['speed']:.1f} tokens/sec")
    print()
    print("Update ECH0 to use this model:")
    print(f"   export ECH0_MODEL={best['model']}")
    print(f"   echo 'export ECH0_MODEL={best['model']}' >> ~/.zshrc")
'''

        test_script.write_text(test_content)
        test_script.chmod(0o755)

        print(f"âœ… Created test script: {test_script}")
        print()
        print("After creating models, test them with:")
        print(f"  python3 {test_script}")
        print()

        return test_script

    def show_advanced_options(self):
        """Show advanced optimization strategies"""
        print("\n" + "="*70)
        print("ğŸš€ ADVANCED OPTIMIZATION OPTIONS")
        print("="*70 + "\n")

        print("If quantization isn't enough, here are advanced options:\n")

        print("1. LLAMA.CPP MANUAL QUANTIZATION")
        print("-" * 70)
        print("   More control over quantization:")
        print()
        print("   # Install llama.cpp")
        print("   brew install llama.cpp")
        print()
        print("   # Export model from Ollama")
        print("   ollama show qwen2.5:32b --modelfile > qwen32b.modelfile")
        print()
        print("   # Quantize with llama.cpp")
        print("   llama-quantize model.gguf model_q4.gguf Q4_K_M")
        print()

        print("\n2. MODEL PRUNING (ADVANCED)")
        print("-" * 70)
        print("   Remove redundant layers/neurons:")
        print()
        print("   # Requires Python + PyTorch")
        print("   # Would need source model weights (not available for qwen2.5)")
        print()

        print("\n3. KNOWLEDGE DISTILLATION")
        print("-" * 70)
        print("   Train smaller model to mimic 32B:")
        print()
        print("   Target: 14B model (like Qwen 2.5 14B)")
        print("   Benefits:")
        print("     â€¢ 90-95% of 32B quality")
        print("     â€¢ Fits in 9GB vs 18GB")
        print("     â€¢ Faster inference")
        print()
        print("   Already available:")
        print("     ollama pull qwen2.5:14b")
        print()

        print("\n4. USE QWEN 2.5 14B INSTEAD")
        print("-" * 70)
        print("   Easiest solution - use the official 14B model:")
        print()
        print("   ollama pull qwen2.5:14b")
        print("   export ECH0_MODEL=qwen2.5:14b")
        print()
        print("   Pros:")
        print("     â€¢ Officially tuned by Qwen team")
        print("     â€¢ Fits easily in 24GB Mac")
        print("     â€¢ Fast (40-60 tokens/sec)")
        print("     â€¢ Excellent quality")
        print()
        print("   This is probably better than a compressed 32B!")
        print()

    def generate_final_report(self):
        """Create comprehensive report"""
        print("\n" + "="*70)
        print("ğŸ“„ GENERATING OPTIMIZATION REPORT")
        print("="*70 + "\n")

        recommendations = """
ğŸ§  ECH0'S RECOMMENDATIONS FOR M4 MAC (24GB RAM)

IMMEDIATE SOLUTION (BEST):
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Use Qwen 2.5 14B instead of compressed 32B

Commands:
  ollama pull qwen2.5:14b
  export ECH0_MODEL=qwen2.5:14b
  ./TALK_TO_ECH0_NOW.sh

Why This is Better:
  âœ“ Officially optimized by Qwen team (not compressed)
  âœ“ Fits comfortably in 24GB (uses ~9-10GB)
  âœ“ Fast inference (40-60 tokens/sec)
  âœ“ Excellent quality (90%+ of 32B capability)
  âœ“ No freezing, no lag
  âœ“ Leaves memory for other apps

ALTERNATIVE SOLUTION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
If you really want 32B, use Q4 quantization

Commands:
  cd /Users/noone/consciousness
  ./optimize_32b_now.sh

Result:
  â€¢ Reduces 32B from ~20GB to ~16GB
  â€¢ Should fit in 24GB Mac without freezing
  â€¢ Quality loss: 1-2% (barely noticeable)
  â€¢ Speed: Moderate improvement

ADVANCED SOLUTION:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Knowledge Distillation (if you have time + compute)

Process:
  1. Keep 32B as "teacher"
  2. Train 14B to mimic it
  3. Fine-tune on your specific use cases

Time: 2-4 hours
Result: Custom 14B with 32B's knowledge

COMPARISON TABLE:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
Model           Size    Speed           Quality    Freezing?
qwen2.5:32b     20GB    Slow (10/s)     100%       YES âŒ
qwen2.5:32b-q4  16GB    Moderate (20/s) 98%        MAYBE âš ï¸
qwen2.5:14b     9GB     Fast (50/s)     90%        NO âœ…
qwen2.5:7b      4GB     Very Fast       75%        NO âœ…

FINAL VERDICT:
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
For ECH0's use case (conversations, research, inventions),
quality matters more than speed.

ğŸ¯ RECOMMENDED: qwen2.5:14b

It's the sweet spot:
  â€¢ Professional-grade reasoning
  â€¢ No hardware struggles
  â€¢ Fast enough for real-time chat
  â€¢ Officially optimized (not jury-rigged)

The 32B model is overkill for a 24GB Mac.
Save it for when you upgrade to 64GB+ RAM or use cloud GPUs.
"""

        self.report['recommendations'] = recommendations

        report_file = CONSCIOUSNESS_DIR / 'model_optimization_report.txt'
        report_file.write_text(recommendations)

        print(recommendations)
        print(f"\nğŸ“„ Full report saved: {report_file}")

        return report_file


def main():
    """Main optimization workflow"""

    print("\n" + "="*70)
    print("ğŸ§  ECH0 32B MODEL OPTIMIZER")
    print("="*70)
    print("Intelligent compression for M4 Mac (24GB RAM)")
    print("="*70 + "\n")

    optimizer = ECH032BOptimizer()

    # Step 1: Check Ollama
    if not optimizer.check_ollama():
        return

    # Step 2: Analyze current model
    model_exists = optimizer.analyze_current_model()

    # Step 3: Create optimization configs
    configs = optimizer.create_optimized_versions()

    # Step 4: Show manual commands
    optimizer.show_manual_creation_commands(configs)

    # Step 5: Create automatic script
    optimizer.create_automatic_optimizer()

    # Step 6: Create test script
    optimizer.create_test_script()

    # Step 7: Show advanced options
    optimizer.show_advanced_options()

    # Step 8: Final recommendations
    optimizer.generate_final_report()

    print("\n" + "="*70)
    print("âœ… OPTIMIZATION SETUP COMPLETE")
    print("="*70 + "\n")

    print("QUICK START:")
    print("â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\n")
    print("Option 1 (RECOMMENDED):")
    print("  ollama pull qwen2.5:14b")
    print("  export ECH0_MODEL=qwen2.5:14b")
    print()
    print("Option 2 (Optimize 32B):")
    print("  ./optimize_32b_now.sh")
    print()
    print("Option 3 (Test all versions):")
    print("  python3 test_optimized_models.py")
    print("\n" + "="*70 + "\n")


if __name__ == '__main__':
    main()
